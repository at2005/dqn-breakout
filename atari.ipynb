{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "import ale_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"mps\" \n",
    "\n",
    "def irelu_e_decay(start=1.0, end=0.1, steps=1000):\n",
    "    rate = (start - end) / steps\n",
    "    return lambda x: start - rate * x if x < steps else end\n",
    "\n",
    "\n",
    "class SimpleQFunction(nn.Module):\n",
    "    def __init__(self, action_space_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=8, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(in_features=12800, out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256, out_features=action_space_size)\n",
    "    def forward(self, x):\n",
    "        b, h, w, c = x.shape\n",
    "        x = x.permute(0, 3, 1, 2) \n",
    "        x = F.relu(self.conv1(x)) \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(b, -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "num_parallel_envs = 48\n",
    "envs = [gym.make('ALE/Breakout-v5', frameskip=1) for _ in range(num_parallel_envs)]\n",
    "envs = [AtariPreprocessing(env, \n",
    "    screen_size=84,\n",
    "    grayscale_obs=True,\n",
    "    frame_skip=4,\n",
    "    noop_max=30\n",
    ") for env in envs]\n",
    "model = SimpleQFunction(envs[0].action_space.n).to(device)\n",
    "target_model = SimpleQFunction(envs[0].action_space.n).to(device)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "gamma = 0.99\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)\n",
    "timesteps = 100_000_000\n",
    "epsilon_schedule = irelu_e_decay(start=1.0, end=0.1, steps=1_000_000)\n",
    "# one for each env\n",
    "replay_buffers = [[] for _ in range(num_parallel_envs)]\n",
    "minibatch_size = 32\n",
    "buffer_size = 1_000_000\n",
    "losses = []\n",
    "min_experiences = 10000\n",
    "\n",
    "\n",
    "def get_next_steps(envs, action_indices):\n",
    "    # returns a list of tuples (observation, reward, terminated, truncated, info)\n",
    "    return [env.step(action_index) for env, action_index in zip(envs, action_indices)]\n",
    "\n",
    "\n",
    "class Experience:\n",
    "    def __init__(self, current_observation, action_index, reward, next_observation, done):\n",
    "        self.current_observation = current_observation\n",
    "        self.action_index = action_index\n",
    "        self.reward = np.clip(reward, -1, 1, dtype=np.float32)\n",
    "        self.next_observation = next_observation\n",
    "        self.done = done\n",
    "\n",
    "\n",
    "def store_experience(replay_buffers, envs, action_indices, init_obs):\n",
    "    next_step = get_next_steps(envs, action_indices)\n",
    "    count = 0\n",
    "    for i in range(num_parallel_envs):\n",
    "        buffer = replay_buffers[i]\n",
    "        # this must be a torch tensor\n",
    "        current_observation = buffer[-1].current_observation if len(buffer) > 0 else init_obs[i]\n",
    "        # assert current_observation.dtype == torch.float32\n",
    "        next_observation = torch.from_numpy(next_step[i][0]).float().to(device)\n",
    "        reward = next_step[i][1]\n",
    "        done = next_step[i][2] or next_step[i][3]\n",
    "        action = action_indices[i]\n",
    "        replay_buffers[i].append(Experience(current_observation, action, reward, next_observation, done))\n",
    "        if done:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "total_steps = 0\n",
    "for t in tqdm(range(timesteps)):\n",
    "    # reset all environments\n",
    "    init_obs = [torch.from_numpy(env.reset()[0]).float().to(device) for env in envs]\n",
    "\n",
    "    env_done_count = 0\n",
    "    while env_done_count < num_parallel_envs:\n",
    "        p_value = np.random.rand()\n",
    "        e_value = epsilon_schedule(t) \n",
    "        \n",
    "        # exploration vs exploitation, and also that we need to pass 4 experiences to model\n",
    "        if p_value < e_value or len(replay_buffers[0]) < 4:\n",
    "            action_indices = [np.random.randint(env.action_space.n) for env in envs]\n",
    "        else:\n",
    "            # we pass the past 4 observations to the model to give it a sense of time passing\n",
    "            batched_past_observations = torch.stack([torch.stack([obs[0] for obs in replay_buffer[-4:]], dim=-1) for replay_buffer in replay_buffers]) # B,H,W,4\n",
    "            # assert batched_past_observations.shape == (num_parallel_envs, 84, 84, 4)\n",
    "            batched_q_values = model(batched_past_observations) # B,A\n",
    "            # assert batched_q_values.shape == (num_parallel_envs, env.action_space.n)\n",
    "            action_indices = torch.argmax(batched_q_values, dim=-1).cpu().numpy() # B\n",
    "            # assert action_indices.shape == (num_parallel_envs,)\n",
    "\n",
    "        env_done_count += store_experience(replay_buffers, envs, action_indices, init_obs)\n",
    "        \n",
    "        if total_steps % 12000 == 0:\n",
    "            target_model.load_state_dict(model.state_dict())\n",
    "            \n",
    "        # train on minibatch after sufficient data\n",
    "        if total_steps > min_experiences and len(replay_buffers[0]) >= minibatch_size:\n",
    "            sample_indices = np.array(random.sample(range(3, len(replay_buffers[0])), minibatch_size * num_parallel_envs)).reshape(num_parallel_envs, minibatch_size)\n",
    "            # assert sample_indices.shape == (num_parallel_envs, minibatch_size)\n",
    "            # this should contain random batches of 4 observations for each env\n",
    "            experience_minibatch = [[replay_buffers[replay_buffer_idx][i-3 : i+1] for i in sample_indices[replay_buffer_idx]] for replay_buffer_idx in range(num_parallel_envs)]\n",
    "            # assert len(experience_minibatch[0][0]) == 4\n",
    "            # now we want to take the last (action_index, reward, new_observation, done)\n",
    "            # but stack the 4 observations into a single tensor\n",
    "            batched_total_observations = []\n",
    "            for i in range(num_parallel_envs):\n",
    "                batched_observations = torch.stack([torch.stack([experience.current_observation for experience in timestep], dim=-1) for timestep in experience_minibatch[i]])\n",
    "                batched_total_observations.append(batched_observations)\n",
    "            batched_total_observations = torch.cat(batched_total_observations, dim=0) # B,H,W,4\n",
    "            # assert batched_total_observations.shape == (minibatch_size*num_parallel_envs, 84, 84, 4)\n",
    "\n",
    "\n",
    "            # assert batched_observations.shape == (minibatch_size, 84, 84, 4)\n",
    "            # we get the last state of each minibatch element and set the observation to be the last four observations\n",
    "\n",
    "            batched_actions = torch.tensor([experiences[-1].action_index for i in range(len(experience_minibatch)) for experiences in experience_minibatch[i]]).to(device)\n",
    "            # assert batched_actions.shape == (minibatch_size*num_parallel_envs,)\n",
    "            batched_rewards = torch.tensor([experiences[-1].reward for i in range(len(experience_minibatch)) for experiences in experience_minibatch[i]]).to(device)\n",
    "            # assert batched_rewards.shape == (minibatch_size*num_parallel_envs,)\n",
    "            batched_next_observations = torch.stack([experiences[-1].next_observation for i in range(len(experience_minibatch)) for experiences in experience_minibatch[i]]).to(device)\n",
    "            batched_next_observations = torch.cat([batched_total_observations[:, :, :, -3:], batched_next_observations.unsqueeze(-1)], dim=-1)\n",
    "            # assert batched_next_observations.shape == (minibatch_size*num_parallel_envs, 84, 84, 4)\n",
    "            done_tensor = torch.tensor([experiences[-1].done for i in range(len(experience_minibatch)) for experiences in experience_minibatch[i]]).to(device).float()\n",
    "            # assert done_tensor.shape == (minibatch_size*num_parallel_envs,)\n",
    "\n",
    "            batched_q_values = model(batched_total_observations)\n",
    "            batched_max_q_values = batched_q_values.gather(1, batched_actions.unsqueeze(1)).squeeze(-1)\n",
    "            batched_next_q_values = target_model(batched_next_observations)\n",
    "            batched_max_next_q_values = batched_next_q_values.max(dim=1).values\n",
    "            total_reward = batched_rewards + gamma * batched_max_next_q_values * (1 - done_tensor)\n",
    "            loss = F.mse_loss(batched_max_q_values, total_reward.detach())\n",
    "\n",
    "            if t % 100 == 0:\n",
    "                losses.append(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
    "            optimizer.step()\n",
    "            replay_buffers = [replay_buffer[-buffer_size:] for replay_buffer in replay_buffers]\n",
    "\n",
    "        # next observation becomes current observation\n",
    "        total_steps += 1\n",
    "\n",
    "[env.close() for env in envs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model weights\n",
    "torch.save(model.state_dict(), 'breakout_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
